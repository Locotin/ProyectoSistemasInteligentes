{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola mundo\n"
     ]
    }
   ],
   "source": [
    "print(\"hola mundo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importaci√≥n de bibliotecas**\n",
    "\n",
    "A continuaci√≥n, se describen las bibliotecas utilizadas en el desarrollo del modelo:\n",
    "\n",
    "- **os**: Permite interactuar con el sistema operativo, √∫til para manejar archivos y directorios en la carga de datos.\n",
    "- **torch**: Librer√≠a principal de PyTorch para el desarrollo y entrenamiento de modelos de aprendizaje profundo.\n",
    "- **torch.nn**: Proporciona m√≥dulos y funciones para construir redes neuronales en PyTorch.\n",
    "- **torch.optim**: Contiene optimizadores utilizados para entrenar modelos mediante descenso de gradiente.\n",
    "- **pandas**: Se utiliza para manipulaci√≥n y an√°lisis de datos tabulares, como la carga de etiquetas desde archivos CSV.\n",
    "- **torchvision.transforms**: Ofrece transformaciones para preprocesar im√°genes antes de ingresarlas al modelo.\n",
    "- **PIL (Python Imaging Library)**: Se usa para cargar y manipular im√°genes en diferentes formatos.\n",
    "- **torch.utils.data (Dataset y DataLoader)**: Facilita la gesti√≥n de los conjuntos de datos y su alimentaci√≥n al modelo en lotes.\n",
    "- **itertools**: Proporciona herramientas para iteraciones eficientes, √∫til para combinaciones o permutaciones de datos.\n",
    "- **torchvision.models (efficientnet_v2_s)**: Se importa el modelo preentrenado EfficientNetV2-S, que se usar√° como base para la clasificaci√≥n.\n",
    "- **torch.nn.functional**: Contiene funciones √∫tiles como activaciones, p√©rdidas y operaciones sobre tensores.\n",
    "\n",
    "### **Uso en el notebook**\n",
    "Estas bibliotecas permiten construir un pipeline de aprendizaje profundo que incluye:\n",
    "1. **Carga y preprocesamiento de datos**: Se usa `os`, `pandas`, `PIL`, y `torchvision.transforms` para leer im√°genes y etiquetas.\n",
    "2. **Definici√≥n del modelo**: Se emplea `torch.nn` y `efficientnet_v2_s` para crear la red neuronal.\n",
    "3. **Entrenamiento**: `torch.optim` y `torch.nn.functional` ayudan a definir el proceso de optimizaci√≥n y la funci√≥n de p√©rdida.\n",
    "4. **Evaluaci√≥n y predicci√≥n**: Se aplican las herramientas de PyTorch para evaluar el modelo y generar predicciones sobre datos de prueba.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools\n",
    "from torchvision.models import efficientnet_v2_s\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Carga y organizaci√≥n de datos**\n",
    "\n",
    "En esta secci√≥n, organizamos los conjuntos de datos para entrenamiento, validaci√≥n y prueba.  \n",
    "Los datos provienen del desaf√≠o **BreastPathQ 2019** y est√°n estructurados en im√°genes de parches histol√≥gicos junto con sus etiquetas.\n",
    "\n",
    "### **1Ô∏è Directorios de im√°genes**\n",
    "- **`train_image_dir`**: Contiene las im√°genes de entrenamiento con sus etiquetas.\n",
    "- **`val_image_dir`**: Contiene las im√°genes de validaci√≥n con sus etiquetas.\n",
    "- **`test_image_dir`**: Contiene las im√°genes de prueba (sin etiquetas).\n",
    "\n",
    "### **2Ô∏è Rutas de etiquetas**\n",
    "- **`train_label_path`**: Archivo CSV con etiquetas de entrenamiento.\n",
    "- **`val_label_path`**: Archivo CSV con etiquetas de validaci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "## **Procesamiento de los conjuntos de datos**\n",
    "\n",
    "1. **Entrenamiento (`df_train`)**\n",
    "   - Se carga el CSV de entrenamiento.\n",
    "   - Se genera el nombre de cada imagen combinando `slide` y `rid`.\n",
    "  \n",
    "2. **Validaci√≥n (`df_val`)**\n",
    "   - Se listan todas las im√°genes del directorio de validaci√≥n.\n",
    "   - Se extrae el `slide` y `rid` del nombre del archivo.\n",
    "   - Se unen estos datos con el CSV de etiquetas.\n",
    "\n",
    "3. **Prueba (`df_test`)**\n",
    "   - Se listan todas las im√°genes del directorio de prueba.\n",
    "   - Se extrae el `slide` y `rid` del nombre del archivo.\n",
    "   - **(Importante)** No hay etiquetas en este conjunto.\n",
    "\n",
    "---\n",
    "\n",
    "## **Diagrama de relaci√≥n entre conjuntos de datos**\n",
    "Para visualizar mejor c√≥mo se organizan los datos, usamos diagramas de Venn:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organizaci√≥n Final de Conjuntos:\n",
      "Train: 2394 im√°genes\n",
      "Validation: 185 im√°genes\n",
      "Test_patches: 1119 im√°genes (Debe ser 1119)\n"
     ]
    }
   ],
   "source": [
    "# Directorios de im√°genes\n",
    "train_image_dir = \"datasets/SPIE_BreastPathQ2019_Training_Validation/breastpathq/datasets/train\"\n",
    "val_image_dir = \"datasets/SPIE_BreastPathQ2019_Training_Validation/breastpathq/datasets/validation\"\n",
    "test_image_dir = \"datasets/SPIE_BreastPathQ2019_Testing/breastpathq-test/test_patches\"\n",
    "\n",
    "# Rutas de etiquetas\n",
    "train_label_path = \"datasets/SPIE_BreastPathQ2019_Training_Validation/breastpathq/datasets/train_labels.csv\"\n",
    "val_label_path = \"datasets/SPIE_BreastPathQ2019_Testing/breastpathq-test/val_labels.csv\"\n",
    "\n",
    "# Cargar datos\n",
    "df_train = pd.read_csv(train_label_path)\n",
    "df_train[\"image_name\"] = df_train[\"slide\"].astype(str) + \"_\" + df_train[\"rid\"].astype(str) + \".tif\"\n",
    "\n",
    "val_images = [f for f in os.listdir(val_image_dir) if f.endswith(\".tif\")]\n",
    "df_val = pd.DataFrame({\"image_name\": val_images})\n",
    "df_val[[\"slide\", \"rid\"]] = df_val[\"image_name\"].str.extract(r'(\\d+)_(\\d+).tif').astype(int)\n",
    "df_val_labels = pd.read_csv(val_label_path)\n",
    "df_val = df_val.merge(df_val_labels, on=[\"slide\", \"rid\"], how=\"left\")\n",
    "\n",
    "test_images = [f for f in os.listdir(test_image_dir) if f.endswith(\".tif\")]\n",
    "df_test = pd.DataFrame({\"image_name\": test_images})\n",
    "df_test[[\"slide\", \"rid\"]] = df_test[\"image_name\"].str.extract(r'(\\d+)_(\\d+).tif').astype(int)\n",
    "\n",
    "# Resumen de conjuntos de datos\n",
    "print(\"Organizaci√≥n Final de Conjuntos:\")\n",
    "print(f\"Train: {len(df_train)} im√°genes\")\n",
    "print(f\"Validation: {len(df_val)} im√°genes\")\n",
    "print(f\"Test_patches: {len(df_test)} im√°genes (Debe ser 1119)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transform=None, is_test=False):\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx][\"image_name\"]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.is_test:\n",
    "            return image, img_name\n",
    "        else:\n",
    "            label = self.df.iloc[idx][\"y\"]\n",
    "            return image, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# Transformaciones para EfficientNetV2\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Crear datasets y dataloaders\n",
    "train_dataset = BreastDataset(df_train, train_image_dir, transform)\n",
    "val_dataset = BreastDataset(df_val, val_image_dir, transform)\n",
    "test_dataset = BreastDataset(df_test, test_image_dir, transform, is_test=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Dispositivo en uso: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üìå Dispositivo en uso: {device}\")\n",
    "\n",
    "efficientnet = efficientnet_v2_s(weights=\"IMAGENET1K_V1\")\n",
    "num_ftrs = efficientnet.classifier[1].in_features\n",
    "efficientnet.classifier[1] = nn.Linear(num_ftrs, 1)  # Modificar la capa final para regresi√≥n\n",
    "efficientnet = efficientnet.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(efficientnet.parameters(), lr=0.0008)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå √âpoca 1/50 - P√©rdida: 0.0424\n",
      "üìå √âpoca 2/50 - P√©rdida: 0.0176\n",
      "üìå √âpoca 3/50 - P√©rdida: 0.0123\n",
      "üìå √âpoca 4/50 - P√©rdida: 0.0099\n",
      "üìå √âpoca 5/50 - P√©rdida: 0.0085\n",
      "üìå √âpoca 6/50 - P√©rdida: 0.0064\n",
      "üìå √âpoca 7/50 - P√©rdida: 0.0055\n",
      "üìå √âpoca 8/50 - P√©rdida: 0.0054\n",
      "üìå √âpoca 9/50 - P√©rdida: 0.0052\n",
      "üìå √âpoca 10/50 - P√©rdida: 0.0041\n",
      "üìå √âpoca 11/50 - P√©rdida: 0.0036\n",
      "üìå √âpoca 12/50 - P√©rdida: 0.0037\n",
      "üìå √âpoca 13/50 - P√©rdida: 0.0049\n",
      "üìå √âpoca 14/50 - P√©rdida: 0.0054\n",
      "üìå √âpoca 15/50 - P√©rdida: 0.0046\n",
      "üìå √âpoca 16/50 - P√©rdida: 0.0033\n",
      "üìå √âpoca 17/50 - P√©rdida: 0.0025\n",
      "üìå √âpoca 18/50 - P√©rdida: 0.0023\n",
      "üìå √âpoca 19/50 - P√©rdida: 0.0022\n",
      "üìå √âpoca 20/50 - P√©rdida: 0.0022\n",
      "üìå √âpoca 21/50 - P√©rdida: 0.0041\n",
      "üìå √âpoca 22/50 - P√©rdida: 0.0118\n",
      "üìå √âpoca 23/50 - P√©rdida: 0.0106\n",
      "üìå √âpoca 24/50 - P√©rdida: 0.0056\n",
      "üìå √âpoca 25/50 - P√©rdida: 0.0041\n",
      "üìå √âpoca 26/50 - P√©rdida: 0.0028\n",
      "üìå √âpoca 27/50 - P√©rdida: 0.0026\n",
      "üìå √âpoca 28/50 - P√©rdida: 0.0027\n",
      "üìå √âpoca 29/50 - P√©rdida: 0.0022\n",
      "üìå √âpoca 30/50 - P√©rdida: 0.0019\n",
      "üìå √âpoca 31/50 - P√©rdida: 0.0019\n",
      "üìå √âpoca 32/50 - P√©rdida: 0.0029\n",
      "üìå √âpoca 33/50 - P√©rdida: 0.0072\n",
      "üìå √âpoca 34/50 - P√©rdida: 0.0065\n",
      "üìå √âpoca 35/50 - P√©rdida: 0.0051\n",
      "üìå √âpoca 36/50 - P√©rdida: 0.0035\n",
      "üìå √âpoca 37/50 - P√©rdida: 0.0022\n",
      "üìå √âpoca 38/50 - P√©rdida: 0.0018\n",
      "üìå √âpoca 39/50 - P√©rdida: 0.0017\n",
      "üìå √âpoca 40/50 - P√©rdida: 0.0018\n",
      "üìå √âpoca 41/50 - P√©rdida: 0.0014\n",
      "üìå √âpoca 42/50 - P√©rdida: 0.0012\n",
      "üìå √âpoca 43/50 - P√©rdida: 0.0016\n",
      "üìå √âpoca 44/50 - P√©rdida: 0.0023\n",
      "üìå √âpoca 45/50 - P√©rdida: 0.0025\n",
      "üìå √âpoca 46/50 - P√©rdida: 0.0085\n",
      "üìå √âpoca 47/50 - P√©rdida: 0.0078\n",
      "üìå √âpoca 48/50 - P√©rdida: 0.0046\n",
      "üìå √âpoca 49/50 - P√©rdida: 0.0033\n",
      "üìå √âpoca 50/50 - P√©rdida: 0.0020\n"
     ]
    }
   ],
   "source": [
    "epochs = 50  # Ajustamos a 50 ya que EfficientNetV2 aprende m√°s r√°pido\n",
    "for epoch in range(epochs):\n",
    "    efficientnet.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = efficientnet(images).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"üìå √âpoca {epoch+1}/{epochs} - P√©rdida: {epoch_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo de predicciones generado: submission_test.csv\n"
     ]
    }
   ],
   "source": [
    "efficientnet.eval()\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, image_names in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = efficientnet(images).squeeze().cpu().numpy()\n",
    "        outputs = F.sigmoid(torch.tensor(outputs)).numpy()  # Convertir a [0,1]\n",
    "\n",
    "        for img_name, pred in zip(image_names, outputs):\n",
    "            slide, rid = img_name.replace(\".tif\", \"\").split(\"_\")\n",
    "            test_predictions.append([int(slide), int(rid), pred])\n",
    "\n",
    "df_test_predictions = pd.DataFrame(test_predictions, columns=[\"slide\", \"rid\", \"score\"])\n",
    "df_test_predictions.to_csv(\"submission_test.csv\", index=False)\n",
    "\n",
    "print(f\"‚úÖ Archivo de predicciones generado: submission_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå PK Score: 0.9085\n"
     ]
    }
   ],
   "source": [
    "def calculate_pk(labels, predictions):\n",
    "    P, Q, T = 0, 0, 0\n",
    "    for (pred_i, true_i), (pred_j, true_j) in itertools.combinations(zip(predictions, labels), 2):\n",
    "        if (true_i < true_j and pred_i < pred_j) or (true_i > true_j and pred_i > pred_j):\n",
    "            P += 1\n",
    "        elif (true_i < true_j and pred_i > pred_j) or (true_i > true_j and pred_i < pred_j):\n",
    "            Q += 1\n",
    "        elif pred_i == pred_j:\n",
    "            T += 1\n",
    "    return (((P - Q) / (P + Q + T)) + 1) / 2 if (P + Q + T) != 0 else 0\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    all_labels, all_predictions = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "    pk = calculate_pk(all_labels, all_predictions)\n",
    "    print(f\"üìå PK Score: {pk:.4f}\")\n",
    "\n",
    "evaluate_model(efficientnet, val_loader, criterion, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sisisvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
