{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola mundo\n"
     ]
    }
   ],
   "source": [
    "print(\"hola mundo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ImportaciÃ³n de bibliotecas**\n",
    "\n",
    "A continuaciÃ³n, se describen las bibliotecas utilizadas en el desarrollo del modelo:\n",
    "\n",
    "- **os**: Permite interactuar con el sistema operativo, Ãºtil para manejar archivos y directorios en la carga de datos.\n",
    "- **torch**: LibrerÃ­a principal de PyTorch para el desarrollo y entrenamiento de modelos de aprendizaje profundo.\n",
    "- **torch.nn**: Proporciona mÃ³dulos y funciones para construir redes neuronales en PyTorch.\n",
    "- **torch.optim**: Contiene optimizadores utilizados para entrenar modelos mediante descenso de gradiente.\n",
    "- **pandas**: Se utiliza para manipulaciÃ³n y anÃ¡lisis de datos tabulares, como la carga de etiquetas desde archivos CSV.\n",
    "- **torchvision.transforms**: Ofrece transformaciones para preprocesar imÃ¡genes antes de ingresarlas al modelo.\n",
    "- **PIL (Python Imaging Library)**: Se usa para cargar y manipular imÃ¡genes en diferentes formatos.\n",
    "- **torch.utils.data (Dataset y DataLoader)**: Facilita la gestiÃ³n de los conjuntos de datos y su alimentaciÃ³n al modelo en lotes.\n",
    "- **itertools**: Proporciona herramientas para iteraciones eficientes, Ãºtil para combinaciones o permutaciones de datos.\n",
    "- **torchvision.models (efficientnet_v2_s)**: Se importa el modelo preentrenado EfficientNetV2-S, que se usarÃ¡ como base para la clasificaciÃ³n.\n",
    "- **torch.nn.functional**: Contiene funciones Ãºtiles como activaciones, pÃ©rdidas y operaciones sobre tensores.\n",
    "\n",
    "### **Uso en el notebook**\n",
    "Estas bibliotecas permiten construir un pipeline de aprendizaje profundo que incluye:\n",
    "1. **Carga y preprocesamiento de datos**: Se usa `os`, `pandas`, `PIL`, y `torchvision.transforms` para leer imÃ¡genes y etiquetas.\n",
    "2. **DefiniciÃ³n del modelo**: Se emplea `torch.nn` y `efficientnet_v2_s` para crear la red neuronal.\n",
    "3. **Entrenamiento**: `torch.optim` y `torch.nn.functional` ayudan a definir el proceso de optimizaciÃ³n y la funciÃ³n de pÃ©rdida.\n",
    "4. **EvaluaciÃ³n y predicciÃ³n**: Se aplican las herramientas de PyTorch para evaluar el modelo y generar predicciones sobre datos de prueba.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools\n",
    "from torchvision.models import efficientnet_v2_s\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Carga y organizaciÃ³n de datos**\n",
    "\n",
    "En esta secciÃ³n, organizamos los conjuntos de datos para entrenamiento, validaciÃ³n y prueba.  \n",
    "Los datos provienen del desafÃ­o **BreastPathQ 2019** y estÃ¡n estructurados en imÃ¡genes de parches histolÃ³gicos junto con sus etiquetas.\n",
    "\n",
    "### **1ï¸ Directorios de imÃ¡genes**\n",
    "- **`train_image_dir`**: Contiene las imÃ¡genes de entrenamiento con sus etiquetas.\n",
    "- **`val_image_dir`**: Contiene las imÃ¡genes de validaciÃ³n con sus etiquetas.\n",
    "- **`test_image_dir`**: Contiene las imÃ¡genes de prueba (sin etiquetas).\n",
    "\n",
    "### **2ï¸ Rutas de etiquetas**\n",
    "- **`train_label_path`**: Archivo CSV con etiquetas de entrenamiento.\n",
    "- **`val_label_path`**: Archivo CSV con etiquetas de validaciÃ³n.\n",
    "\n",
    "---\n",
    "\n",
    "## **Procesamiento de los conjuntos de datos**\n",
    "\n",
    "1. **Entrenamiento (`df_train`)**\n",
    "   - Se carga el CSV de entrenamiento.\n",
    "   - Se genera el nombre de cada imagen combinando `slide` y `rid`.\n",
    "  \n",
    "2. **ValidaciÃ³n (`df_val`)**\n",
    "   - Se listan todas las imÃ¡genes del directorio de validaciÃ³n.\n",
    "   - Se extrae el `slide` y `rid` del nombre del archivo.\n",
    "   - Se unen estos datos con el CSV de etiquetas.\n",
    "\n",
    "3. **Prueba (`df_test`)**\n",
    "   - Se listan todas las imÃ¡genes del directorio de prueba.\n",
    "   - Se extrae el `slide` y `rid` del nombre del archivo.\n",
    "   - **(Importante)** No hay etiquetas en este conjunto.\n",
    "\n",
    "---\n",
    "\n",
    "## **Diagrama de relaciÃ³n entre conjuntos de datos**\n",
    "Para visualizar mejor cÃ³mo se organizan los datos, usamos diagramas de Venn:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrganizaciÃ³n Final de Conjuntos:\n",
      "Train: 2394 imÃ¡genes\n",
      "Validation: 185 imÃ¡genes\n",
      "Test_patches: 1119 imÃ¡genes (Debe ser 1119)\n"
     ]
    }
   ],
   "source": [
    "# Directorios de imÃ¡genes\n",
    "train_image_dir = \"datasets/SPIE_BreastPathQ2019_Training_Validation/breastpathq/datasets/train\"\n",
    "val_image_dir = \"datasets/SPIE_BreastPathQ2019_Training_Validation/breastpathq/datasets/validation\"\n",
    "test_image_dir = \"datasets/SPIE_BreastPathQ2019_Testing/breastpathq-test/test_patches\"\n",
    "\n",
    "# Rutas de etiquetas\n",
    "train_label_path = \"datasets/SPIE_BreastPathQ2019_Training_Validation/breastpathq/datasets/train_labels.csv\"\n",
    "val_label_path = \"datasets/SPIE_BreastPathQ2019_Testing/breastpathq-test/val_labels.csv\"\n",
    "\n",
    "# Cargar datos\n",
    "df_train = pd.read_csv(train_label_path)\n",
    "df_train[\"image_name\"] = df_train[\"slide\"].astype(str) + \"_\" + df_train[\"rid\"].astype(str) + \".tif\"\n",
    "\n",
    "val_images = [f for f in os.listdir(val_image_dir) if f.endswith(\".tif\")]\n",
    "df_val = pd.DataFrame({\"image_name\": val_images})\n",
    "df_val[[\"slide\", \"rid\"]] = df_val[\"image_name\"].str.extract(r'(\\d+)_(\\d+).tif').astype(int)\n",
    "df_val_labels = pd.read_csv(val_label_path)\n",
    "df_val = df_val.merge(df_val_labels, on=[\"slide\", \"rid\"], how=\"left\")\n",
    "\n",
    "test_images = [f for f in os.listdir(test_image_dir) if f.endswith(\".tif\")]\n",
    "df_test = pd.DataFrame({\"image_name\": test_images})\n",
    "df_test[[\"slide\", \"rid\"]] = df_test[\"image_name\"].str.extract(r'(\\d+)_(\\d+).tif').astype(int)\n",
    "\n",
    "# Resumen de conjuntos de datos\n",
    "print(\"OrganizaciÃ³n Final de Conjuntos:\")\n",
    "print(f\"Train: {len(df_train)} imÃ¡genes\")\n",
    "print(f\"Validation: {len(df_val)} imÃ¡genes\")\n",
    "print(f\"Test_patches: {len(df_test)} imÃ¡genes (Debe ser 1119)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transform=None, is_test=False):\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx][\"image_name\"]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.is_test:\n",
    "            return image, img_name\n",
    "        else:\n",
    "            label = self.df.iloc[idx][\"y\"]\n",
    "            return image, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# Transformaciones para EfficientNetV2\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Crear datasets y dataloaders\n",
    "train_dataset = BreastDataset(df_train, train_image_dir, transform)\n",
    "val_dataset = BreastDataset(df_val, val_image_dir, transform)\n",
    "test_dataset = BreastDataset(df_test, test_image_dir, transform, is_test=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Dispositivo en uso: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ“Œ Dispositivo en uso: {device}\")\n",
    "\n",
    "efficientnet = efficientnet_v2_s(weights=\"IMAGENET1K_V1\")\n",
    "num_ftrs = efficientnet.classifier[1].in_features\n",
    "efficientnet.classifier[1] = nn.Linear(num_ftrs, 1)  # Modificar la capa final para regresiÃ³n\n",
    "efficientnet = efficientnet.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(efficientnet.parameters(), lr=0.0008)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Ã‰poca 1/50 - PÃ©rdida: 0.0424\n",
      "ğŸ“Œ Ã‰poca 2/50 - PÃ©rdida: 0.0176\n",
      "ğŸ“Œ Ã‰poca 3/50 - PÃ©rdida: 0.0123\n",
      "ğŸ“Œ Ã‰poca 4/50 - PÃ©rdida: 0.0099\n",
      "ğŸ“Œ Ã‰poca 5/50 - PÃ©rdida: 0.0085\n",
      "ğŸ“Œ Ã‰poca 6/50 - PÃ©rdida: 0.0064\n",
      "ğŸ“Œ Ã‰poca 7/50 - PÃ©rdida: 0.0055\n",
      "ğŸ“Œ Ã‰poca 8/50 - PÃ©rdida: 0.0054\n",
      "ğŸ“Œ Ã‰poca 9/50 - PÃ©rdida: 0.0052\n",
      "ğŸ“Œ Ã‰poca 10/50 - PÃ©rdida: 0.0041\n",
      "ğŸ“Œ Ã‰poca 11/50 - PÃ©rdida: 0.0036\n",
      "ğŸ“Œ Ã‰poca 12/50 - PÃ©rdida: 0.0037\n",
      "ğŸ“Œ Ã‰poca 13/50 - PÃ©rdida: 0.0049\n",
      "ğŸ“Œ Ã‰poca 14/50 - PÃ©rdida: 0.0054\n",
      "ğŸ“Œ Ã‰poca 15/50 - PÃ©rdida: 0.0046\n",
      "ğŸ“Œ Ã‰poca 16/50 - PÃ©rdida: 0.0033\n",
      "ğŸ“Œ Ã‰poca 17/50 - PÃ©rdida: 0.0025\n",
      "ğŸ“Œ Ã‰poca 18/50 - PÃ©rdida: 0.0023\n",
      "ğŸ“Œ Ã‰poca 19/50 - PÃ©rdida: 0.0022\n",
      "ğŸ“Œ Ã‰poca 20/50 - PÃ©rdida: 0.0022\n",
      "ğŸ“Œ Ã‰poca 21/50 - PÃ©rdida: 0.0041\n",
      "ğŸ“Œ Ã‰poca 22/50 - PÃ©rdida: 0.0118\n",
      "ğŸ“Œ Ã‰poca 23/50 - PÃ©rdida: 0.0106\n",
      "ğŸ“Œ Ã‰poca 24/50 - PÃ©rdida: 0.0056\n",
      "ğŸ“Œ Ã‰poca 25/50 - PÃ©rdida: 0.0041\n",
      "ğŸ“Œ Ã‰poca 26/50 - PÃ©rdida: 0.0028\n",
      "ğŸ“Œ Ã‰poca 27/50 - PÃ©rdida: 0.0026\n",
      "ğŸ“Œ Ã‰poca 28/50 - PÃ©rdida: 0.0027\n",
      "ğŸ“Œ Ã‰poca 29/50 - PÃ©rdida: 0.0022\n",
      "ğŸ“Œ Ã‰poca 30/50 - PÃ©rdida: 0.0019\n",
      "ğŸ“Œ Ã‰poca 31/50 - PÃ©rdida: 0.0019\n",
      "ğŸ“Œ Ã‰poca 32/50 - PÃ©rdida: 0.0029\n",
      "ğŸ“Œ Ã‰poca 33/50 - PÃ©rdida: 0.0072\n",
      "ğŸ“Œ Ã‰poca 34/50 - PÃ©rdida: 0.0065\n",
      "ğŸ“Œ Ã‰poca 35/50 - PÃ©rdida: 0.0051\n",
      "ğŸ“Œ Ã‰poca 36/50 - PÃ©rdida: 0.0035\n",
      "ğŸ“Œ Ã‰poca 37/50 - PÃ©rdida: 0.0022\n",
      "ğŸ“Œ Ã‰poca 38/50 - PÃ©rdida: 0.0018\n",
      "ğŸ“Œ Ã‰poca 39/50 - PÃ©rdida: 0.0017\n",
      "ğŸ“Œ Ã‰poca 40/50 - PÃ©rdida: 0.0018\n",
      "ğŸ“Œ Ã‰poca 41/50 - PÃ©rdida: 0.0014\n",
      "ğŸ“Œ Ã‰poca 42/50 - PÃ©rdida: 0.0012\n",
      "ğŸ“Œ Ã‰poca 43/50 - PÃ©rdida: 0.0016\n",
      "ğŸ“Œ Ã‰poca 44/50 - PÃ©rdida: 0.0023\n",
      "ğŸ“Œ Ã‰poca 45/50 - PÃ©rdida: 0.0025\n",
      "ğŸ“Œ Ã‰poca 46/50 - PÃ©rdida: 0.0085\n",
      "ğŸ“Œ Ã‰poca 47/50 - PÃ©rdida: 0.0078\n",
      "ğŸ“Œ Ã‰poca 48/50 - PÃ©rdida: 0.0046\n",
      "ğŸ“Œ Ã‰poca 49/50 - PÃ©rdida: 0.0033\n",
      "ğŸ“Œ Ã‰poca 50/50 - PÃ©rdida: 0.0020\n"
     ]
    }
   ],
   "source": [
    "epochs = 50  # Ajustamos a 50 ya que EfficientNetV2 aprende mÃ¡s rÃ¡pido\n",
    "for epoch in range(epochs):\n",
    "    efficientnet.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = efficientnet(images).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"ğŸ“Œ Ã‰poca {epoch+1}/{epochs} - PÃ©rdida: {epoch_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Archivo de predicciones generado: submission_test.csv\n"
     ]
    }
   ],
   "source": [
    "efficientnet.eval()\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, image_names in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = efficientnet(images).squeeze().cpu().numpy()\n",
    "        outputs = F.sigmoid(torch.tensor(outputs)).numpy()  # Convertir a [0,1]\n",
    "\n",
    "        for img_name, pred in zip(image_names, outputs):\n",
    "            slide, rid = img_name.replace(\".tif\", \"\").split(\"_\")\n",
    "            test_predictions.append([int(slide), int(rid), pred])\n",
    "\n",
    "df_test_predictions = pd.DataFrame(test_predictions, columns=[\"slide\", \"rid\", \"score\"])\n",
    "df_test_predictions.to_csv(\"submission_test.csv\", index=False)\n",
    "\n",
    "print(f\"âœ… Archivo de predicciones generado: submission_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ PK Score: 0.9085\n"
     ]
    }
   ],
   "source": [
    "def calculate_pk(labels, predictions):\n",
    "    P, Q, T = 0, 0, 0\n",
    "    for (pred_i, true_i), (pred_j, true_j) in itertools.combinations(zip(predictions, labels), 2):\n",
    "        if (true_i < true_j and pred_i < pred_j) or (true_i > true_j and pred_i > pred_j):\n",
    "            P += 1\n",
    "        elif (true_i < true_j and pred_i > pred_j) or (true_i > true_j and pred_i < pred_j):\n",
    "            Q += 1\n",
    "        elif pred_i == pred_j:\n",
    "            T += 1\n",
    "    return (((P - Q) / (P + Q + T)) + 1) / 2 if (P + Q + T) != 0 else 0\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    all_labels, all_predictions = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "    pk = calculate_pk(all_labels, all_predictions)\n",
    "    print(f\"ğŸ“Œ PK Score: {pk:.4f}\")\n",
    "\n",
    "evaluate_model(efficientnet, val_loader, criterion, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sisisvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
